{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5509b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7d08ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sachin', 'is', 'considered', 'to', 'be', 'one', 'of', 'the', '.', 'greatest', 'cricket', 'players', '.', 'Virat', 'is', 'the', 'captain', 'of', 'the', 'Indian', 'cricket', 'team']\n",
      "['Sachin is considered to be one of the.', 'greatest cricket players.', 'Virat is the captain of the Indian cricket team']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "\n",
    "sent = \"Sachin is considered to be one of the.  greatest cricket players. Virat is the captain of the Indian cricket team\"\n",
    "\n",
    "\n",
    "print(word_tokenize(sent))  # Sepatred in word array\n",
    "print(sent_tokenize(sent))  # Separated in sentence array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6aaa969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  # Corpus in stopword\n",
    "\n",
    "\n",
    "import nltk    \n",
    "nltk.download('stopwords')         # Download it\n",
    "\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')   # Store english words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecfa045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the unclean version :  ['Sachin', 'is', 'considered', 'to', 'be', 'one', 'of', 'the', '.', 'greatest', 'cricket', 'players', '.', 'Virat', 'is', 'the', 'captain', 'of', 'the', 'Indian', 'cricket', 'team']\n",
      "This is the cleaned version :  ['Sachin', 'considered', 'one', '.', 'greatest', 'cricket', 'players', '.', 'Virat', 'captain', 'Indian', 'cricket', 'team']\n"
     ]
    }
   ],
   "source": [
    "token = word_tokenize(sent)   # Old sentence i.e. token \n",
    "\n",
    "\n",
    "\n",
    "cleaned_token = []            # New cleaned token\n",
    "\n",
    "\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "         cleaned_token.append(word)  #Appending necessary words only to cleaned_token\n",
    "            \n",
    "# Difference between cleaned and old token            \n",
    "print(\"This is the unclean version : \",token)             \n",
    "print(\"This is the cleaned version : \",cleaned_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150ae8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making words in lower \n",
    "\n",
    "words = [\n",
    "    cleaned_token.lower() \n",
    "    for cleaned_token in cleaned_token \n",
    "        if cleaned_token.isalpha()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7355797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sachin', 'considered', 'one', 'greatest', 'cricket', 'players', 'virat', 'captain', 'indian', 'cricket', 'team']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7071a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sachin', 'consid', 'one', 'greatest', 'cricket', 'player', 'virat', 'captain', 'indian', 'cricket', 'team']\n"
     ]
    }
   ],
   "source": [
    "# Making the words in to root form\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "port_stemmer_output = [stemmer.stem(words) for words in words]\n",
    "\n",
    "\n",
    "\n",
    "print(port_stemmer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bcec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading omw-1.4: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sachin', 'considered', 'one', 'greatest', 'cricket', 'player', 'virat', 'captain', 'indian', 'cricket', 'team']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_output = [lemmatizer.lemmatize(words) for words in words]\n",
    "print(lemmatizer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49cb4108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sachin', 'NNP'), ('considered', 'VBD'), ('one', 'CD'), ('.', '.'), ('greatest', 'JJS'), ('cricket', 'NN'), ('players', 'NNS'), ('.', '.'), ('Virat', 'NNP'), ('captain', 'NN'), ('Indian', 'JJ'), ('cricket', 'NN'), ('team', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "         cleaned_token.append(word)\n",
    "tagged = pos_tag(cleaned_token)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b716162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Create representation of document by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f39eb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'fit_ransform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10324\\1360781732.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Compute the TF-IDF representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtfidf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_ransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Get the feature names (words)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'fit_ransform'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute the TF-IDF representation\n",
    "tfidf_matrix = vectorizer.fit_ransform(documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Print the feature names\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fecc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5becb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [ \"Sachin is considered to be one of the greatest cricket players\",\n",
    " \"Federer is considered one of the greatest tennis players\",\n",
    " \"Nadal is considered one of the greatest tennis players\",\n",
    " \"Virat is the captain of the Indian cricket team\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5504d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer = \"word\", norm = None , use_idf = True , smooth_idf=True)\n",
    "Mat = vectorizer.fit(docs)\n",
    "print(Mat.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "         cleaned_token.append(word)\n",
    "print(\"This is the unclean version : \",token)\n",
    "print(\"This is the cleaned version : \",cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfMat = vectorizer.fit_transform(docs)\n",
    "print(tfidfMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd166c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = vectorizer.get_feature_names_out()\n",
    "print(features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9715c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
